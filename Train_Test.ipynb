{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suNxLn5TY58-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import math\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ============================================================================\n",
        "# GEORGIAN KEYBOARD LAYOUT - For realistic misclick simulation\n",
        "# ============================================================================\n",
        "#ა ბ გ დ ე ვ ზ თ ი კ ლ მ ნ ო პ ჟ რ ს ტ უ ფ ქ ღ ყ შ ჩ ც ძ წ ჭ ხ ჯ ჰ\n",
        "# Standard Georgian QWERTY keyboard layout (approximation)\n",
        "GEORGIAN_KEYBOARD = {\n",
        "   \"ა\" : ['ქ','ს,','ზ'],\n",
        "   'ბ' : ['ვ','ნ','გ','ჰ'], # we can also add space but not sure that model is going to work for this one\n",
        "   'გ' : ['ვ','ბ','ფ','ტ','ყ','ჰ'],\n",
        "   'დ' : ['ხ','ც','ს','ფ','რ','ე'],\n",
        "   'ე' : ['წ','რ','დ','ს'],\n",
        "   'ვ': ['ც','ბ','ფ','გ'],\n",
        "   'ზ': ['ა','ს','ხ'],\n",
        "   \"თ\": ['ღ','ყ','ფ','გ','ტ','რ'], # pressed/not pressed shift\n",
        "   'ი': ['უ','ო','ჯ','კ'],\n",
        "   'კ': ['მ','ჯ','ლ','ი','ო'],\n",
        "   'ლ': ['კ','ო','პ'],\n",
        "   'მ': ['ნ','ჯ','კ','ლ'],\n",
        "   'ნ': ['ბ','ჰ','ჯ'], #no მ\n",
        "   'ო': ['ი','პ','კ','ლ'],\n",
        "   'პ': ['ო','ლ'],\n",
        "   'ჟ': ['ჯ','ჰ','უ','ნ','მ'],\n",
        "   'რ': ['ღ','ე','ტ','თ','დ','ფ'],\n",
        "   'ს': ['შ','ა','ზ','ხ','წ','ე'],\n",
        "   'ტ': ['რ','ყ','ფ','გ'],\n",
        "   'უ': ['ყ','ჰ','ჯ','ი'],\n",
        "   'ფ': ['ც','ვ','დ','გ','რ','ტ'],\n",
        "   'ქ': ['ა','წ'],\n",
        "   'ღ': ['თ','რ','ტ','ე','დ','ფ'],\n",
        "   'ყ': ['ტ','გ','ჰ','უ'],\n",
        "   'შ': ['ს','ა','დ','წ','ე','ხ'],\n",
        "   'ჩ': ['ც','ხ','ვ','დ','ფ'],\n",
        "   'ც': ['ხ','ვ','დ','ფ'],\n",
        "   'ძ': ['ა', 'ს', 'ხ'],\n",
        "   'წ': ['ქ', 'ე', 'ს', 'ა'],\n",
        "   'ჭ': ['ქ', 'ე', 'ს', 'ა'],\n",
        "   'ხ': ['ა', 'ს', 'დ', 'ც','ზ'],\n",
        "   'ჯ': ['ჰ', 'უ', 'ი', 'კ', 'მ', 'ნ'],\n",
        "   'ჰ': ['გ', 'ყ', 'უ', 'ჯ', 'ნ', 'ბ']\n",
        "}\n",
        "\n",
        "# Around line 107 - Update this section:\n",
        "ALL_GEORGIAN_CHARS = set()\n",
        "\n",
        "# Define your data directory once\n",
        "DATA_DIR = 'drive/MyDrive/data' # Update this path as needed, i have trained on colab current one would be data\n",
        "\n",
        "for word_file in ['wordsChunk_0.json', 'wordsChunk_1.json', 'wordsChunk_2.json']:\n",
        "    file_path = Path(DATA_DIR) / word_file\n",
        "    if file_path.exists():\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            words = json.load(f)\n",
        "            for word in words[:1000]:\n",
        "                ALL_GEORGIAN_CHARS.update(word)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# INFERENCE - Generate corrected spellings\n",
        "# ============================================================================\n",
        "\n",
        "def correct_word(model, word, vocab, device='cuda', max_len=100):\n",
        "    \"\"\"\n",
        "    Correct a single misspelled word using LSTM decoder.\n",
        "\n",
        "    Uses greedy decoding with early stopping to prevent loops.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Encode input (no SOS for source)\n",
        "    src = torch.LongTensor([vocab.encode(word, add_sos=False, add_eos=True)]).to(device)\n",
        "    src_lengths = torch.LongTensor([src.size(1)])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode source\n",
        "        encoder_outputs, hidden, cell = model.encoder(src, src_lengths)\n",
        "\n",
        "        # Bridge encoder hidden to decoder\n",
        "        hidden_combined = []\n",
        "        cell_combined = []\n",
        "        for i in range(model.decoder.num_layers):\n",
        "            h_forward = hidden[i*2]\n",
        "            h_backward = hidden[i*2 + 1]\n",
        "            h_combined = torch.cat([h_forward, h_backward], dim=1)\n",
        "            hidden_combined.append(model.bridge_h(h_combined))\n",
        "\n",
        "            c_forward = cell[i*2]\n",
        "            c_backward = cell[i*2 + 1]\n",
        "            c_combined = torch.cat([c_forward, c_backward], dim=1)\n",
        "            cell_combined.append(model.bridge_c(c_combined))\n",
        "\n",
        "        hidden = torch.stack(hidden_combined)\n",
        "        cell = torch.stack(cell_combined)\n",
        "\n",
        "        # Start with SOS token\n",
        "        tgt_token = torch.LongTensor([[vocab.char2idx['<SOS>']]]).to(device)\n",
        "        decoded_tokens = []\n",
        "\n",
        "        # Greedy decoding with loop prevention\n",
        "        for step in range(max_len):\n",
        "            prediction, hidden, cell, _ = model.decoder(\n",
        "                tgt_token, hidden, cell, encoder_outputs, mask=None\n",
        "            )\n",
        "\n",
        "            # Get most likely next token\n",
        "            next_token_id = prediction.argmax(dim=-1).item()\n",
        "\n",
        "            # Stop conditions\n",
        "            if next_token_id == vocab.char2idx['<EOS>']:\n",
        "                break\n",
        "            if next_token_id == vocab.char2idx['<PAD>']:\n",
        "                break\n",
        "            if next_token_id == vocab.char2idx['<SOS>']:\n",
        "                continue  # Skip if model predicts SOS again\n",
        "\n",
        "            # Prevent infinite loops: stop if output is too long relative to input\n",
        "            if len(decoded_tokens) > len(word) * 3:\n",
        "                break\n",
        "\n",
        "            # Detect repetition: if last 3 characters are same, stop\n",
        "            if len(decoded_tokens) >= 3:\n",
        "                last_three = decoded_tokens[-3:]\n",
        "                if len(set(last_three)) == 1:  # All same character\n",
        "                    break\n",
        "\n",
        "            decoded_tokens.append(next_token_id)\n",
        "            tgt_token = torch.LongTensor([[next_token_id]]).to(device)\n",
        "\n",
        "    # Decode tokens to string\n",
        "    return vocab.decode(decoded_tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgSooEDAFsdf",
        "outputId": "0243f373-3809-4b49-e04d-5b8b772a025e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "n-Gk3GnPuggN"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DATA CORRUPTION - Simulating realistic typing errors\n",
        "# ============================================================================\n",
        "\n",
        "def corrupt_word(word, corruption_prob=1.0):\n",
        "    \"\"\"\n",
        "    Apply realistic corruptions to simulate typing errors.\n",
        "\n",
        "    ALWAYS corrupts words (100% rate) so model learns to actually correct,\n",
        "    not just copy input to output.\n",
        "\n",
        "    Error Types:\n",
        "    1. Character-level errors:\n",
        "       - Substitution: Adjacent key typo (35%)\n",
        "       - Deletion: Missing character (25%)\n",
        "       - Insertion: Extra character (20%)\n",
        "       - Transposition: Swapped characters (15%)\n",
        "       - Repetition: Doubled character (5%)\n",
        "    \"\"\"\n",
        "    # Skip very short words\n",
        "    if len(word) < 2:\n",
        "        return word\n",
        "\n",
        "    original_word = word\n",
        "    max_attempts = 10  # Try harder to corrupt\n",
        "\n",
        "    for attempt in range(max_attempts):\n",
        "        word_list = list(original_word)\n",
        "\n",
        "        # Number of errors: 1-3 based on word length\n",
        "        if len(word_list) <= 4:\n",
        "            num_errors = 1\n",
        "        elif len(word_list) <= 8:\n",
        "            num_errors = random.randint(1, 2)\n",
        "        else:\n",
        "            num_errors = random.randint(1, 3)\n",
        "\n",
        "        for _ in range(num_errors):\n",
        "            if len(word_list) < 2:\n",
        "                break\n",
        "\n",
        "            error_type = random.choices(\n",
        "                ['substitute', 'delete', 'insert', 'transpose', 'repeat'],\n",
        "                weights=[0.35, 0.25, 0.20, 0.15, 0.05]\n",
        "            )[0]\n",
        "\n",
        "            pos = random.randint(0, len(word_list) - 1)\n",
        "\n",
        "            if error_type == 'substitute':\n",
        "                char = word_list[pos]\n",
        "                if char in GEORGIAN_KEYBOARD and GEORGIAN_KEYBOARD[char]:\n",
        "                    word_list[pos] = random.choice(GEORGIAN_KEYBOARD[char])\n",
        "                elif ALL_GEORGIAN_CHARS:\n",
        "                    # Pick a random different character\n",
        "                    candidates = [c for c in ALL_GEORGIAN_CHARS if c != char]\n",
        "                    if candidates:\n",
        "                        word_list[pos] = random.choice(candidates)\n",
        "\n",
        "            elif error_type == 'delete':\n",
        "                if len(word_list) > 2:\n",
        "                    word_list.pop(pos)\n",
        "\n",
        "            elif error_type == 'insert':\n",
        "                if ALL_GEORGIAN_CHARS:\n",
        "                    if pos > 0 and random.random() < 0.3:\n",
        "                        # Duplicate adjacent char (common typo)\n",
        "                        word_list.insert(pos, word_list[pos-1])\n",
        "                    else:\n",
        "                        word_list.insert(pos, random.choice(list(ALL_GEORGIAN_CHARS)))\n",
        "\n",
        "            elif error_type == 'transpose':\n",
        "                if pos < len(word_list) - 1:\n",
        "                    word_list[pos], word_list[pos + 1] = word_list[pos + 1], word_list[pos]\n",
        "\n",
        "            elif error_type == 'repeat':\n",
        "                # Double a character\n",
        "                word_list.insert(pos, word_list[pos])\n",
        "\n",
        "        corrupted = ''.join(word_list)\n",
        "\n",
        "        # Success if we actually changed the word\n",
        "        if corrupted != original_word and len(corrupted) > 0:\n",
        "            return corrupted\n",
        "\n",
        "    # Last resort: force a substitution\n",
        "    if len(original_word) >= 2 and ALL_GEORGIAN_CHARS:\n",
        "        word_list = list(original_word)\n",
        "        pos = random.randint(0, len(word_list) - 1)\n",
        "        candidates = [c for c in ALL_GEORGIAN_CHARS if c != word_list[pos]]\n",
        "        if candidates:\n",
        "            word_list[pos] = random.choice(candidates)\n",
        "            return ''.join(word_list)\n",
        "\n",
        "    return original_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1jd8dLPFDvdE"
      },
      "outputs": [],
      "source": [
        "def load_and_corrupt_data(data_dir='data', corruption_rate=1.0, max_words=None):\n",
        "    \"\"\"\n",
        "    Load words from JSON files and create corrupted versions.\n",
        "\n",
        "    NOW: 100% corruption rate - every word gets corrupted so model\n",
        "    learns to actually CORRECT, not just COPY.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing word JSON files\n",
        "        corruption_rate: Percentage of words to corrupt (1.0 = 100%)\n",
        "        max_words: Maximum number of words to load (None = all)\n",
        "\n",
        "    Returns:\n",
        "        List of (corrupted, correct) tuples\n",
        "    \"\"\"\n",
        "    all_words = []\n",
        "    data_path = Path(data_dir)\n",
        "\n",
        "    # Load all word chunks\n",
        "    for word_file in ['wordsChunk_0.json', 'wordsChunk_1.json', 'wordsChunk_2.json']:\n",
        "        file_path = data_path / word_file\n",
        "        if file_path.exists():\n",
        "            print(f\"Loading {word_file}...\")\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                words = json.load(f)\n",
        "                all_words.extend(words)\n",
        "\n",
        "    # Limit dataset size if specified\n",
        "    if max_words:\n",
        "        all_words = all_words[:max_words]\n",
        "\n",
        "    # Remove very short words (hard to corrupt meaningfully)\n",
        "    all_words = [w for w in all_words if len(w) >= 2]\n",
        "\n",
        "    print(f\"Loaded {len(all_words)} words total (after filtering short words)\")\n",
        "\n",
        "    # Create training pairs: (corrupted, correct)\n",
        "    training_pairs = []\n",
        "    failed_corruptions = 0\n",
        "\n",
        "    for word in tqdm(all_words, desc=\"Corrupting words\"):\n",
        "        corrupted = corrupt_word(word, corruption_prob=corruption_rate)\n",
        "\n",
        "        # Only add if we successfully corrupted (corrupted != original)\n",
        "        if corrupted != word:\n",
        "            training_pairs.append((corrupted, word))\n",
        "        else:\n",
        "            failed_corruptions += 1\n",
        "            # Still add as identity pair (some clean examples help)\n",
        "            if random.random() < 0.1:  # Keep 10% of failed corruptions\n",
        "                training_pairs.append((word, word))\n",
        "\n",
        "    # Count actual corruptions\n",
        "    num_corrupted = sum(1 for c, o in training_pairs if c != o)\n",
        "    print(f\"Successfully corrupted: {num_corrupted}/{len(training_pairs)} \"\n",
        "          f\"({100*num_corrupted/len(training_pairs):.1f}%)\")\n",
        "    print(f\"Failed to corrupt: {failed_corruptions} words\")\n",
        "\n",
        "    # Shuffle to mix corrupted and clean examples\n",
        "    random.shuffle(training_pairs)\n",
        "\n",
        "    return training_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Z3fb9UAPt2Bf"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CHARACTER VOCABULARY - Building character-to-index mappings\n",
        "# ============================================================================\n",
        "\n",
        "class CharVocab:\n",
        "    \"\"\"Character-level vocabulary for Georgian text.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.PAD_TOKEN = '<PAD>'\n",
        "        self.SOS_TOKEN = '<SOS>'  # Start of sequence\n",
        "        self.EOS_TOKEN = '<EOS>'  # End of sequence\n",
        "        self.UNK_TOKEN = '<UNK>'  # Unknown character\n",
        "\n",
        "        self.char2idx = {\n",
        "            self.PAD_TOKEN: 0,\n",
        "            self.SOS_TOKEN: 1,\n",
        "            self.EOS_TOKEN: 2,\n",
        "            self.UNK_TOKEN: 3,\n",
        "        }\n",
        "        self.idx2char = {v: k for k, v in self.char2idx.items()}\n",
        "        self.next_idx = 4\n",
        "\n",
        "    def build_vocab(self, words):\n",
        "        \"\"\"Build vocabulary from list of words.\"\"\"\n",
        "        for word in words:\n",
        "            for char in word:\n",
        "                if char not in self.char2idx:\n",
        "                    self.char2idx[char] = self.next_idx\n",
        "                    self.idx2char[self.next_idx] = char\n",
        "                    self.next_idx += 1\n",
        "        print(f\"Vocabulary size: {len(self.char2idx)}\")\n",
        "        return self\n",
        "\n",
        "    def encode(self, text, add_sos=False, add_eos=True):\n",
        "        \"\"\"Convert text to list of indices.\"\"\"\n",
        "        indices = []\n",
        "        if add_sos:\n",
        "            indices.append(self.char2idx[self.SOS_TOKEN])\n",
        "        indices.extend([self.char2idx.get(char, self.char2idx[self.UNK_TOKEN])\n",
        "                        for char in text])\n",
        "        if add_eos:\n",
        "            indices.append(self.char2idx[self.EOS_TOKEN])\n",
        "        return indices\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert list of indices to text.\"\"\"\n",
        "        chars = []\n",
        "        for idx in indices:\n",
        "            if idx == self.char2idx[self.EOS_TOKEN]:\n",
        "                break\n",
        "            if idx == self.char2idx[self.PAD_TOKEN]:\n",
        "                continue\n",
        "            if idx == self.char2idx[self.SOS_TOKEN]:\n",
        "                continue\n",
        "            chars.append(self.idx2char.get(idx, self.UNK_TOKEN))\n",
        "        return ''.join(chars)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.char2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3QaOdDudEAu_"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DATASET - PyTorch Dataset for character sequences\n",
        "# ============================================================================\n",
        "\n",
        "class SpellingDataset(Dataset):\n",
        "    \"\"\"Dataset for spelling correction pairs.\"\"\"\n",
        "\n",
        "    def __init__(self, pairs, vocab, max_len=50):\n",
        "        self.pairs = pairs\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        corrupted, correct = self.pairs[idx]\n",
        "\n",
        "        # Encode sequences\n",
        "        # Source: no SOS, just EOS\n",
        "        src = self.vocab.encode(corrupted, add_sos=False, add_eos=True)\n",
        "        # Target: add SOS at start, EOS at end for teacher forcing\n",
        "        tgt = self.vocab.encode(correct, add_sos=True, add_eos=True)\n",
        "\n",
        "        # Truncate if too long\n",
        "        src = src[:self.max_len]\n",
        "        tgt = tgt[:self.max_len]\n",
        "\n",
        "        return torch.LongTensor(src), torch.LongTensor(tgt)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to pad sequences in batch.\"\"\"\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "\n",
        "    # Pad sequences\n",
        "    src_padded = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
        "    tgt_padded = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
        "\n",
        "    return src_padded, tgt_padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VWmTbZoaEH8c"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LSTM ENCODER-DECODER MODEL - With attention mechanism\n",
        "# ============================================================================\n",
        "\n",
        "class LSTMEncoder(nn.Module):\n",
        "    \"\"\"LSTM Encoder for sequence encoding.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_lengths=None):\n",
        "        # src: [batch, seq_len]\n",
        "        embedded = self.dropout(self.embedding(src))  # [batch, seq_len, emb_dim]\n",
        "\n",
        "        if src_lengths is not None:\n",
        "            # Pack padded sequences for efficiency\n",
        "            packed = nn.utils.rnn.pack_padded_sequence(\n",
        "                embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "            )\n",
        "            outputs, (hidden, cell) = self.lstm(packed)\n",
        "            outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "        else:\n",
        "            outputs, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        # outputs: [batch, seq_len, hidden_dim*2] (bidirectional)\n",
        "        # hidden: [num_layers*2, batch, hidden_dim]\n",
        "        # cell: [num_layers*2, batch, hidden_dim]\n",
        "        return outputs, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "046DzeD8EAxp"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    \"\"\"Bahdanau (additive) attention mechanism.\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim, encoder_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim + encoder_dim, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask=None):\n",
        "        # hidden: [batch, hidden_dim]\n",
        "        # encoder_outputs: [batch, src_len, encoder_dim]\n",
        "\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        src_len = encoder_outputs.size(1)\n",
        "\n",
        "        # Repeat hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch, src_len, hidden_dim]\n",
        "\n",
        "        # Calculate attention scores\n",
        "        energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)  # [batch, src_len]\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            attention = attention.masked_fill(mask == 0, -1e4)\n",
        "\n",
        "        # Apply softmax\n",
        "        attention_weights = torch.softmax(attention, dim=1)  # [batch, src_len]\n",
        "\n",
        "        # Apply attention to encoder outputs\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
        "        context = context.squeeze(1)  # [batch, encoder_dim]\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "\n",
        "class LSTMDecoder(nn.Module):\n",
        "    \"\"\"LSTM Decoder with attention mechanism.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, encoder_dim, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.attention = BahdanauAttention(hidden_dim, encoder_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim + encoder_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_dim + encoder_dim + embedding_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, hidden, cell, encoder_outputs, mask=None):\n",
        "        # tgt: [batch, 1] (one token at a time for autoregressive generation)\n",
        "        # hidden: [num_layers, batch, hidden_dim]\n",
        "        # cell: [num_layers, batch, hidden_dim]\n",
        "        # encoder_outputs: [batch, src_len, encoder_dim]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(tgt))  # [batch, 1, emb_dim]\n",
        "\n",
        "        # Calculate attention using the top layer's hidden state\n",
        "        context, attn_weights = self.attention(\n",
        "            hidden[-1], encoder_outputs, mask\n",
        "        )  # context: [batch, encoder_dim]\n",
        "\n",
        "        # Combine embedded input and context\n",
        "        context = context.unsqueeze(1)  # [batch, 1, encoder_dim]\n",
        "        lstm_input = torch.cat([embedded, context], dim=2)  # [batch, 1, emb_dim + encoder_dim]\n",
        "\n",
        "        # Pass through LSTM\n",
        "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "        # output: [batch, 1, hidden_dim]\n",
        "\n",
        "        # Prepare for output projection\n",
        "        output = output.squeeze(1)  # [batch, hidden_dim]\n",
        "        context = context.squeeze(1)  # [batch, encoder_dim]\n",
        "        embedded = embedded.squeeze(1)  # [batch, emb_dim]\n",
        "\n",
        "        # Concatenate and project to vocabulary\n",
        "        prediction = self.fc_out(\n",
        "            torch.cat([output, context, embedded], dim=1)\n",
        "        )  # [batch, vocab_size]\n",
        "\n",
        "        return prediction, hidden, cell, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JqrTYcKlEVvd"
      },
      "outputs": [],
      "source": [
        "class SpellingLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM Encoder-Decoder with Attention for spelling correction.\n",
        "\n",
        "    Architecture:\n",
        "    - Encoder: 2-layer bidirectional LSTM (256 hidden units per direction)\n",
        "    - Decoder: 2-layer unidirectional LSTM (512 hidden units)\n",
        "    - Attention: Bahdanau (additive) attention mechanism\n",
        "    - Embedding: 256 dimensions\n",
        "\n",
        "    Why LSTM for this task:\n",
        "    1. Sequential nature captures left-to-right character dependencies\n",
        "    2. Attention helps align corrupted → correct characters\n",
        "    3. Bidirectional encoder sees full context of misspelled word\n",
        "    4. Proven effective for sequence-to-sequence tasks\n",
        "    5. Less memory than Transformer for longer sequences\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim=256, encoder_hidden_dim=256,\n",
        "                 decoder_hidden_dim=512, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = LSTMEncoder(\n",
        "            vocab_size, embedding_dim, encoder_hidden_dim, num_layers, dropout\n",
        "        )\n",
        "\n",
        "        # Encoder is bidirectional, so encoder output dim is encoder_hidden_dim * 2\n",
        "        encoder_output_dim = encoder_hidden_dim * 2\n",
        "\n",
        "        self.decoder = LSTMDecoder(\n",
        "            vocab_size, embedding_dim, decoder_hidden_dim,\n",
        "            encoder_output_dim, num_layers, dropout\n",
        "        )\n",
        "\n",
        "        # Bridge from encoder hidden to decoder hidden (bidirectional to unidirectional)\n",
        "        self.bridge_h = nn.Linear(encoder_hidden_dim * 2, decoder_hidden_dim)\n",
        "        self.bridge_c = nn.Linear(encoder_hidden_dim * 2, decoder_hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                if 'lstm' in name:\n",
        "                    nn.init.orthogonal_(param)\n",
        "                else:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "\n",
        "    def forward(self, src, tgt, src_padding_mask=None, tgt_padding_mask=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward pass for training (teacher forcing).\n",
        "\n",
        "        Args:\n",
        "            src: Source sequence (corrupted) [batch, src_len]\n",
        "            tgt: Target sequence (correct) [batch, tgt_len]\n",
        "            src_padding_mask: Mask for source padding [batch, src_len]\n",
        "            tgt_padding_mask: Mask for target padding (not used in LSTM)\n",
        "\n",
        "        Returns:\n",
        "            outputs: Predictions [batch, tgt_len, vocab_size]\n",
        "        \"\"\"\n",
        "        batch_size = src.size(0)\n",
        "        tgt_len = tgt.size(1)\n",
        "\n",
        "        # Calculate source lengths for packing\n",
        "        if src_padding_mask is not None:\n",
        "            src_lengths = (~src_padding_mask).sum(dim=1)\n",
        "        else:\n",
        "            src_lengths = torch.full((batch_size,), src.size(1), dtype=torch.long, device=src.device)\n",
        "\n",
        "        # Encode\n",
        "        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)\n",
        "        # encoder_outputs: [batch, src_len, encoder_hidden*2]\n",
        "        # hidden: [num_layers*2, batch, encoder_hidden]\n",
        "        # cell: [num_layers*2, batch, encoder_hidden]\n",
        "\n",
        "        # Bridge encoder hidden states to decoder (combine forward and backward)\n",
        "        # Take pairs of layers (forward, backward) and combine them\n",
        "        hidden_combined = []\n",
        "        cell_combined = []\n",
        "        for i in range(self.decoder.num_layers):\n",
        "            # Combine forward and backward hidden states\n",
        "            h_forward = hidden[i*2]      # Forward LSTM layer i\n",
        "            h_backward = hidden[i*2 + 1]  # Backward LSTM layer i\n",
        "            h_combined = torch.cat([h_forward, h_backward], dim=1)\n",
        "            hidden_combined.append(self.bridge_h(h_combined))\n",
        "\n",
        "            c_forward = cell[i*2]\n",
        "            c_backward = cell[i*2 + 1]\n",
        "            c_combined = torch.cat([c_forward, c_backward], dim=1)\n",
        "            cell_combined.append(self.bridge_c(c_combined))\n",
        "\n",
        "        hidden = torch.stack(hidden_combined)  # [num_layers, batch, decoder_hidden]\n",
        "        cell = torch.stack(cell_combined)      # [num_layers, batch, decoder_hidden]\n",
        "\n",
        "        # Create mask for attention (inverse of padding mask)\n",
        "        attn_mask = None\n",
        "        if src_padding_mask is not None:\n",
        "            attn_mask = ~src_padding_mask  # [batch, src_len]\n",
        "\n",
        "        # Decode with teacher forcing\n",
        "        outputs = []\n",
        "        for t in range(tgt_len):\n",
        "            tgt_t = tgt[:, t].unsqueeze(1)  # [batch, 1]\n",
        "            prediction, hidden, cell, _ = self.decoder(\n",
        "                tgt_t, hidden, cell, encoder_outputs, attn_mask\n",
        "            )\n",
        "            outputs.append(prediction)\n",
        "\n",
        "        outputs = torch.stack(outputs, dim=1)  # [batch, tgt_len, vocab_size]\n",
        "        return outputs\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        \"\"\"Dummy method for compatibility (not needed for LSTM).\"\"\"\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa-CdXs1EA0h"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TRAINING LOOP - With mixed precision (FP16) and proper monitoring\n",
        "# ============================================================================\n",
        "\n",
        "def train_model(model, train_loader, val_loader, vocab,\n",
        "                num_epochs=10, learning_rate=0.0001, device='cuda'):\n",
        "    \"\"\"\n",
        "    Train the spelling correction model with FP16 mixed precision. otherwise training would take a lifetime, and its okay 16fp for this task.\n",
        "\n",
        "    Uses:\n",
        "    - Adam optimizer with learning rate 1e-4\n",
        "    - CrossEntropyLoss (standard for classification)\n",
        "    - GradScaler for FP16 training\n",
        "    - Learning rate scheduling\n",
        "    - Early stopping based on validation loss\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=2\n",
        "    )\n",
        "\n",
        "    # Loss function with label smoothing for better generalization\n",
        "    criterion = nn.CrossEntropyLoss(\n",
        "        ignore_index=vocab.char2idx['<PAD>'],\n",
        "        label_smoothing=0.1\n",
        "    )\n",
        "\n",
        "    # Mixed precision training\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    max_patience = 5\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for src, tgt in pbar:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            # Teacher forcing: use ground truth as decoder input\n",
        "            tgt_input = tgt[:, :-1]  \n",
        "            tgt_output = tgt[:, 1:]  \n",
        "\n",
        "            # Create masks\n",
        "            src_padding_mask = (src == vocab.char2idx['<PAD>'])\n",
        "            tgt_padding_mask = (tgt_input == vocab.char2idx['<PAD>'])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision forward pass\n",
        "            with autocast():\n",
        "                output = model(src, tgt_input,\n",
        "                             src_padding_mask=src_padding_mask,\n",
        "                             tgt_padding_mask=tgt_padding_mask)\n",
        "\n",
        "                # Reshape for loss calculation\n",
        "                loss = criterion(\n",
        "                    output.reshape(-1, len(vocab)),\n",
        "                    tgt_output.reshape(-1)\n",
        "                )\n",
        "\n",
        "            # Backward pass with scaling\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients, capping the magnitude of gradients\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Metrics\n",
        "            train_loss += loss.item()\n",
        "            pred = output.argmax(dim=-1)\n",
        "            mask = (tgt_output != vocab.char2idx['<PAD>'])\n",
        "            train_correct += ((pred == tgt_output) & mask).sum().item()\n",
        "            train_total += mask.sum().item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100*train_correct/train_total:.2f}%'\n",
        "            })\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for src, tgt in val_loader:\n",
        "                src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "                tgt_input = tgt[:, :-1]\n",
        "                tgt_output = tgt[:, 1:]\n",
        "\n",
        "                src_padding_mask = (src == vocab.char2idx['<PAD>'])\n",
        "                tgt_padding_mask = (tgt_input == vocab.char2idx['<PAD>'])\n",
        "\n",
        "                with autocast():\n",
        "                    output = model(src, tgt_input,\n",
        "                                 src_padding_mask=src_padding_mask,\n",
        "                                 tgt_padding_mask=tgt_padding_mask)\n",
        "\n",
        "                    loss = criterion(\n",
        "                        output.reshape(-1, len(vocab)),\n",
        "                        tgt_output.reshape(-1)\n",
        "                    )\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                pred = output.argmax(dim=-1)\n",
        "                mask = (tgt_output != vocab.char2idx['<PAD>'])\n",
        "                val_correct += ((pred == tgt_output) & mask).sum().item()\n",
        "                val_total += mask.sum().item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n",
        "        print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': best_val_loss,\n",
        "            }, 'drive/MyDrive/best_model1.pt')\n",
        "            print(f\"  ✓ Saved best model (val_loss: {best_val_loss:.4f})\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= max_patience:\n",
        "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
        "                break\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUr7gClHEA3D"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# INFERENCE - Generate corrected spellings\n",
        "# ============================================================================\n",
        "\n",
        "def correct_word(model, word, vocab, device='cuda', max_len=100):\n",
        "    \"\"\"\n",
        "    Correct a single misspelled word using LSTM decoder.\n",
        "\n",
        "    Uses greedy decoding with early stopping to prevent loops.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Encode input (no SOS for source)\n",
        "    src = torch.LongTensor([vocab.encode(word, add_sos=False, add_eos=True)]).to(device)\n",
        "    src_lengths = torch.LongTensor([src.size(1)])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode source\n",
        "        encoder_outputs, hidden, cell = model.encoder(src, src_lengths)\n",
        "\n",
        "        # Bridge encoder hidden to decoder\n",
        "        hidden_combined = []\n",
        "        cell_combined = []\n",
        "        for i in range(model.decoder.num_layers):\n",
        "            h_forward = hidden[i*2]\n",
        "            h_backward = hidden[i*2 + 1]\n",
        "            h_combined = torch.cat([h_forward, h_backward], dim=1)\n",
        "            hidden_combined.append(model.bridge_h(h_combined))\n",
        "\n",
        "            c_forward = cell[i*2]\n",
        "            c_backward = cell[i*2 + 1]\n",
        "            c_combined = torch.cat([c_forward, c_backward], dim=1)\n",
        "            cell_combined.append(model.bridge_c(c_combined))\n",
        "\n",
        "        hidden = torch.stack(hidden_combined)\n",
        "        cell = torch.stack(cell_combined)\n",
        "\n",
        "        # Start with SOS token\n",
        "        tgt_token = torch.LongTensor([[vocab.char2idx['<SOS>']]]).to(device)\n",
        "        decoded_tokens = []\n",
        "\n",
        "        # Greedy decoding with loop prevention\n",
        "        for step in range(max_len):\n",
        "            prediction, hidden, cell, _ = model.decoder(\n",
        "                tgt_token, hidden, cell, encoder_outputs, mask=None\n",
        "            )\n",
        "\n",
        "            # Get most likely next token\n",
        "            next_token_id = prediction.argmax(dim=-1).item()\n",
        "\n",
        "            # Stop conditions\n",
        "            if next_token_id == vocab.char2idx['<EOS>']:\n",
        "                break\n",
        "            if next_token_id == vocab.char2idx['<PAD>']:\n",
        "                break\n",
        "            if next_token_id == vocab.char2idx['<SOS>']:\n",
        "                continue  \n",
        "\n",
        "            # Prevent infinite loops: stop if output is too long relative to input\n",
        "            if len(decoded_tokens) > len(word) * 3:\n",
        "                break\n",
        "\n",
        "            # Detect repetition: if last 3 characters are same, stop\n",
        "            if len(decoded_tokens) >= 3:\n",
        "                last_three = decoded_tokens[-3:]\n",
        "                if len(set(last_three)) == 1:  # All same character\n",
        "                    break\n",
        "\n",
        "            decoded_tokens.append(next_token_id)\n",
        "            tgt_token = torch.LongTensor([[next_token_id]]).to(device)\n",
        "\n",
        "    # Decode tokens to string\n",
        "    return vocab.decode(decoded_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrBxWVYYEA6B",
        "outputId": "30202293-37af-4bb7-f2ef-8bddcb20717d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "======================================================================\n",
            "STEP 1: Loading and corrupting data (100% corruption)\n",
            "======================================================================\n",
            "Loading wordsChunk_0.json...\n",
            "Loading wordsChunk_1.json...\n",
            "Loading wordsChunk_2.json...\n",
            "Loaded 271787 words total (after filtering short words)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Corrupting words: 100%|██████████| 271787/271787 [00:01<00:00, 179429.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully corrupted: 271787/271787 (100.0%)\n",
            "Failed to corrupt: 0 words\n",
            "\n",
            "======================================================================\n",
            "STEP 2: Building character vocabulary\n",
            "======================================================================\n",
            "Vocabulary size: 39\n",
            "\n",
            "======================================================================\n",
            "STEP 3: Creating train/validation split\n",
            "======================================================================\n",
            "Train: 244608 pairs\n",
            "Val: 27179 pairs\n",
            "\n",
            "======================================================================\n",
            "STEP 4: Initializing LSTM Encoder-Decoder model\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3798964758.py:32: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 8,476,967\n",
            "\n",
            "======================================================================\n",
            "STEP 5: Training model with FP16 mixed precision\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15:   0%|          | 0/1911 [00:00<?, ?it/s]/tmp/ipython-input-3798964758.py:60: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 1/15: 100%|██████████| 1911/1911 [03:09<00:00, 10.07it/s, loss=1.1637, acc=80.47%]\n",
            "/tmp/ipython-input-3798964758.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 Summary:\n",
            "  Train Loss: 1.2986 | Train Acc: 80.47%\n",
            "  Val Loss: 1.1313 | Val Acc: 85.56%\n",
            "  ✓ Saved best model (val_loss: 1.1313)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/15: 100%|██████████| 1911/1911 [03:10<00:00, 10.04it/s, loss=1.0783, acc=86.81%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2 Summary:\n",
            "  Train Loss: 1.0947 | Train Acc: 86.81%\n",
            "  Val Loss: 1.0610 | Val Acc: 87.73%\n",
            "  ✓ Saved best model (val_loss: 1.0610)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/15: 100%|██████████| 1911/1911 [03:12<00:00,  9.91it/s, loss=1.0769, acc=88.18%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3 Summary:\n",
            "  Train Loss: 1.0486 | Train Acc: 88.18%\n",
            "  Val Loss: 1.0364 | Val Acc: 88.45%\n",
            "  ✓ Saved best model (val_loss: 1.0364)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/15: 100%|██████████| 1911/1911 [03:11<00:00,  9.95it/s, loss=1.0395, acc=89.16%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4 Summary:\n",
            "  Train Loss: 1.0166 | Train Acc: 89.16%\n",
            "  Val Loss: 1.0208 | Val Acc: 88.96%\n",
            "  ✓ Saved best model (val_loss: 1.0208)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/15: 100%|██████████| 1911/1911 [03:12<00:00,  9.95it/s, loss=1.0309, acc=89.90%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5 Summary:\n",
            "  Train Loss: 0.9928 | Train Acc: 89.90%\n",
            "  Val Loss: 1.0103 | Val Acc: 89.29%\n",
            "  ✓ Saved best model (val_loss: 1.0103)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/15: 100%|██████████| 1911/1911 [03:12<00:00,  9.91it/s, loss=0.9725, acc=90.56%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6 Summary:\n",
            "  Train Loss: 0.9720 | Train Acc: 90.56%\n",
            "  Val Loss: 1.0041 | Val Acc: 89.53%\n",
            "  ✓ Saved best model (val_loss: 1.0041)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/15: 100%|██████████| 1911/1911 [03:12<00:00,  9.93it/s, loss=0.9629, acc=91.11%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7 Summary:\n",
            "  Train Loss: 0.9559 | Train Acc: 91.11%\n",
            "  Val Loss: 1.0027 | Val Acc: 89.59%\n",
            "  ✓ Saved best model (val_loss: 1.0027)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/15: 100%|██████████| 1911/1911 [03:12<00:00,  9.93it/s, loss=0.9682, acc=91.63%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8 Summary:\n",
            "  Train Loss: 0.9400 | Train Acc: 91.63%\n",
            "  Val Loss: 1.0028 | Val Acc: 89.64%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/15: 100%|██████████| 1911/1911 [03:09<00:00, 10.10it/s, loss=0.9560, acc=92.03%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9 Summary:\n",
            "  Train Loss: 0.9282 | Train Acc: 92.03%\n",
            "  Val Loss: 1.0018 | Val Acc: 89.69%\n",
            "  ✓ Saved best model (val_loss: 1.0018)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/15: 100%|██████████| 1911/1911 [03:10<00:00, 10.02it/s, loss=0.9335, acc=92.40%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10 Summary:\n",
            "  Train Loss: 0.9168 | Train Acc: 92.40%\n",
            "  Val Loss: 1.0028 | Val Acc: 89.77%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/15: 100%|██████████| 1911/1911 [03:09<00:00, 10.09it/s, loss=0.8944, acc=92.71%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 11 Summary:\n",
            "  Train Loss: 0.9081 | Train Acc: 92.71%\n",
            "  Val Loss: 1.0034 | Val Acc: 89.73%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/15: 100%|██████████| 1911/1911 [03:09<00:00, 10.06it/s, loss=0.9121, acc=93.05%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 12 Summary:\n",
            "  Train Loss: 0.8984 | Train Acc: 93.05%\n",
            "  Val Loss: 1.0060 | Val Acc: 89.65%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/15: 100%|██████████| 1911/1911 [03:09<00:00, 10.10it/s, loss=0.8857, acc=94.10%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 13 Summary:\n",
            "  Train Loss: 0.8690 | Train Acc: 94.10%\n",
            "  Val Loss: 1.0038 | Val Acc: 89.87%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/15: 100%|██████████| 1911/1911 [03:08<00:00, 10.12it/s, loss=0.8399, acc=94.59%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14 Summary:\n",
            "  Train Loss: 0.8558 | Train Acc: 94.59%\n",
            "  Val Loss: 1.0077 | Val Acc: 89.81%\n",
            "\n",
            "Early stopping triggered after 14 epochs\n"
          ]
        }
      ],
      "source": [
        "CORRUPTION_RATE = 1.0  \n",
        "MAX_WORDS = None  \n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 15  \n",
        "LEARNING_RATE = 0.001  \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Step 1: Load and corrupt data\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 1: Loading and corrupting data (100% corruption)\")\n",
        "print(\"=\"*70)\n",
        "pairs = load_and_corrupt_data(\n",
        "    data_dir=DATA_DIR,\n",
        "    corruption_rate=CORRUPTION_RATE,\n",
        "    max_words=MAX_WORDS\n",
        ")\n",
        "\n",
        "# Step 2: Build vocabulary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: Building character vocabulary\")\n",
        "print(\"=\"*70)\n",
        "all_words = [word for pair in pairs for word in pair]\n",
        "vocab = CharVocab().build_vocab(all_words)\n",
        "\n",
        "# Step 3: Split into train/val\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: Creating train/validation split\")\n",
        "print(\"=\"*70)\n",
        "split_idx = int(0.9 * len(pairs))\n",
        "train_pairs = pairs[:split_idx]\n",
        "val_pairs = pairs[split_idx:]\n",
        "print(f\"Train: {len(train_pairs)} pairs\")\n",
        "print(f\"Val: {len(val_pairs)} pairs\")\n",
        "\n",
        "# Step 4: Create datasets and loaders\n",
        "train_dataset = SpellingDataset(train_pairs, vocab)\n",
        "val_dataset = SpellingDataset(val_pairs, vocab)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE,\n",
        "    shuffle=True, collate_fn=collate_fn, num_workers=0\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=BATCH_SIZE,\n",
        "    shuffle=False, collate_fn=collate_fn, num_workers=0\n",
        ")\n",
        "\n",
        "# Step 5: Initialize model\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: Initializing LSTM Encoder-Decoder model\")\n",
        "print(\"=\"*70)\n",
        "model = SpellingLSTM(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=256,\n",
        "    encoder_hidden_dim=256,\n",
        "    decoder_hidden_dim=512,\n",
        "    num_layers=2,\n",
        "    dropout=0.3\n",
        ")\n",
        "# great exploration, i didnt knew these functionionality existed\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model parameters: {num_params:,}\")\n",
        "\n",
        "# Step 6: Train model\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 5: Training model with FP16 mixed precision\")\n",
        "print(\"=\"*70)\n",
        "model = train_model(\n",
        "    model, train_loader, val_loader, vocab,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F0B-Psrg5wN",
        "outputId": "b515df1c-b1e9-4ce6-aae8-fe4b192a4e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 6: Testing on example corrections\n",
            "======================================================================\n",
            "✓ Corrupted: ოქროოიპროიბთ         → Corrected: ოქროპირობით          (True: ოქროპირობით)\n",
            "✓ Corrupted: პ-ეში                → Corrected: პეში                 (True: პეში)\n",
            "✓ Corrupted: გაუჭირვწბლადდ        → Corrected: გაუჭირვებლად         (True: გაუჭირვებლად)\n",
            "✗ Corrupted: ჰლია                 → Corrected: ჰოლია                (True: ჰულია)\n",
            "✗ Corrupted: ხდების               → Corrected: ხედების              (True: ხეების)\n",
            "✗ Corrupted: უკფრორე              → Corrected: უკროდე               (True: უფრორე)\n",
            "✗ Corrupted: სასნელგ              → Corrected: სასნელით             (True: სასჯელთ)\n",
            "✗ Corrupted: გს,სარკლი            → Corrected: გასარკლი             (True: გასარკული)\n",
            "✗ Corrupted: კომოთი               → Corrected: კომოთი               (True: კომოდით)\n",
            "✓ Corrupted: უყვარრადა            → Corrected: უყვარადა             (True: უყვარადა)\n",
            "✗ Corrupted: ჯოლტ                 → Corrected: ჯოლტ                 (True: ნოლტი)\n",
            "✗ Corrupted: კრიტჩტონის           → Corrected: კრიტონის             (True: კრიჩტონის)\n",
            "✗ Corrupted: მონფლიქოტლოჯგ        → Corrected: მონფლიქოლოგ          (True: კონფლიქტოლოგ)\n",
            "✓ Corrupted: აგუტებკ              → Corrected: აგუტები              (True: აგუტები)\n",
            "✓ Corrupted: მხეეცბით             → Corrected: მხეცებით             (True: მხეცებით)\n",
            "\n",
            "Accuracy: 6/15 (40.0%)\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Test on examples\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 6: Testing on example corrections\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load best model\n",
        "checkpoint = torch.load('drive/MyDrive/best_model1.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.to(device)\n",
        "\n",
        "# Test on validation pairs (these are all corrupted now!)\n",
        "test_examples = val_pairs[:15]  # Take 15 examples\n",
        "\n",
        "correct_count = 0\n",
        "for corrupted_word, original_word in test_examples:\n",
        "    corrected = correct_word(model, corrupted_word, vocab, device)\n",
        "    is_correct = corrected == original_word\n",
        "    correct_count += int(is_correct)\n",
        "    status = \"✓\" if is_correct else \"✗\"\n",
        "    print(f\"{status} Corrupted: {corrupted_word:20} → Corrected: {corrected:20} (True: {original_word})\")\n",
        "\n",
        "print(f\"\\nAccuracy: {correct_count}/{len(test_examples)} ({100*correct_count/len(test_examples):.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "there could be some corrected words to be valid, despite the fact they are not for example,\n",
        "✗ Corrupted: ხდების               → Corrected: ხედების              (True: ხეების)\n",
        "also weird words showed up e.g ჯოლტი,აგუტები, top 5 betrayals moments "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdwyiMvDTZ2Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iAmhBvLeA2p"
      },
      "source": [
        "weird words, top 5 betrayal moments in history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec4c4d84",
        "outputId": "bb69e7a6-7563-4dc6-fcd5-0e10e28ea419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a Georgian word to correct (type 'exit' to quit):\n",
            "Your word: გამარჯონა\n",
            "  Original: გამარჯონა\n",
            "  Corrected: გამარჯონა\n",
            "Your word: არსი\n",
            "  Original: არსი\n",
            "  Corrected: არსის\n",
            "Your word: არის\n",
            "  Original: არის\n",
            "  Corrected: არისა\n",
            "Your word: კარგად\n",
            "  Original: კარგად\n",
            "  Corrected: კარგად\n",
            "Your word: ოთახი\n",
            "  Original: ოთახი\n",
            "  Corrected: ოთახის\n",
            "Your word: გამარჰობა\n",
            "  Original: გამარჰობა\n",
            "  Corrected: გამარჯობა\n",
            "Your word: გაგიმარჰოს\n",
            "  Original: გაგიმარჰოს\n",
            "  Corrected: გაგიმართოს\n",
            "Your word: ტერენტ\n",
            "  Original: ტერენტ\n",
            "  Corrected: ტერენტი\n",
            "Your word: შოთ\n",
            "  Original: შოთ\n",
            "  Corrected: შოთ\n",
            "Your word: შოთა\n",
            "  Original: შოთა\n",
            "  Corrected: შოთა\n",
            "Your word: შოთი\n",
            "  Original: შოთი\n",
            "  Corrected: შოთი\n",
            "Your word: კამპუსო\n",
            "  Original: კამპუსო\n",
            "  Corrected: კამპუსი\n",
            "Your word: პროგამა\n",
            "  Original: პროგამა\n",
            "  Corrected: პროგამა\n",
            "Your word: გამარჰონა\n",
            "  Original: გამარჰონა\n",
            "  Corrected: გამარგონა\n",
            "Your word: პროგრმა\n",
            "  Original: პროგრმა\n",
            "  Corrected: პროგრამ\n",
            "Your word: exit\n",
            "Exiting interactive correction.\n"
          ]
        }
      ],
      "source": [
        "# Make sure the model and vocab are loaded from previous steps\n",
        "# If you restarted the kernel, you might need to run the training and setup cells again.\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(\"Enter a Georgian word to correct (type 'exit' to quit):\")\n",
        "while True:\n",
        "    user_input = input(\"Your word: \").strip()\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    if not user_input:\n",
        "        print(\"Please enter a word.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        corrected_word = correct_word(model, user_input, vocab, device)\n",
        "        print(f\"  Original: {user_input}\\n  Corrected: {corrected_word}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        print(\"Please ensure the model and vocabulary are properly loaded.\")\n",
        "\n",
        "print(\"Exiting interactive correction.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "got my name correct, ტერენტ -> ტერენტი :D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE74g3GnTd7F"
      },
      "source": [
        "there are weird words in dataset, that explain why it tries to match the uncorrect words to correct ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm4XOGLthvyu",
        "outputId": "315a3fb3-95c9-4cc5-f5c4-ab6886470101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 2: Building character vocabulary\n",
            "======================================================================\n",
            "Vocabulary size: 39\n",
            "Vocabulary size: 39\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: Building character vocabulary\")\n",
        "print(\"=\"*70)\n",
        "all_words = [word for pair in pairs for word in pair]\n",
        "vocab = CharVocab().build_vocab(all_words)\n",
        "\n",
        "\n",
        "vocab = CharVocab().build_vocab(all_words)\n",
        "\n",
        "vocab_data = {\n",
        "    \"char2idx\": vocab.char2idx,\n",
        "    \"idx2char\": vocab.idx2char,\n",
        "}\n",
        "\n",
        "with open(\"drive/MyDrive/char_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(vocab_data, f, ensure_ascii=False, indent=2)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
