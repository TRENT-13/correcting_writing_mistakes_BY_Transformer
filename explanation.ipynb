{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1bbc759",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Georgian Spelling Correction: Complete Model Explanation\n",
    "\n",
    "## 1. Framing the Problem as a Sequence-to-Sequence Task\n",
    "\n",
    "### The Problem Statement\n",
    "The task is **Georgian spelling correction** — given a misspelled Georgian word (like `გაამრჯობა`), the model should output the correct spelling (`გამარჯობა`).\n",
    "\n",
    "### Why Seq2Seq?\n",
    "This is naturally framed as a **sequence-to-sequence (seq2seq) problem** because:\n",
    "\n",
    "| Input Sequence | Output Sequence |\n",
    "|----------------|-----------------|\n",
    "| `გ → ა → ა → მ → რ → ჯ → ო → ბ → ა` (corrupted) | `გ → ა → მ → ა → რ → ჯ → ო → ბ → ა` (correct) |\n",
    "\n",
    "- **Input and output lengths can differ** — deletions remove characters, insertions add characters\n",
    "- The model must learn **alignment** between corrupted and correct sequences\n",
    "- It's essentially a **translation task** from \"corrupted Georgian\" to \"correct Georgian\"\n",
    "\n",
    "### Character-Level vs Word-Level\n",
    "The code chooses **character-level** processing for several key reasons (from DL_HW_3 (1).ipynb.ipynb#L10-L15)):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2218c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. WHY CHARACTER-LEVEL TRANSFORMER?\n",
    "   - Georgian alphabet has only ~33 letters → small vocabulary (~50 with special tokens)\n",
    "   - Spelling errors happen at character level (typos, adjacent keys)\n",
    "   - Better generalization than word-level models for unseen misspellings\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e739663",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Looking at char_vocab.json, the vocabulary is only **39 tokens** (33 Georgian letters + 4 special tokens + punctuation).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Mechanics of Recurrent Networks at Character Level\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "The model is an **LSTM Encoder-Decoder with Bahdanau Attention**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cce10b",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    SpellingLSTM Architecture                     │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│  ENCODER (Bidirectional LSTM)                                   │\n",
    "│  ┌─────────────────────────────────────────────────────────┐    │\n",
    "│  │  Input: \"გაამრჯობა\" (corrupted)                          │    │\n",
    "│  │  Embedding: 256 dims                                     │    │\n",
    "│  │  LSTM: 2 layers × 256 hidden (bidirectional = 512 out)  │    │\n",
    "│  │  Output: encoder_outputs, (hidden, cell)                │    │\n",
    "│  └─────────────────────────────────────────────────────────┘    │\n",
    "│                            ↓                                    │\n",
    "│  BRIDGE (Linear layers to transform bidirectional → decoder)    │\n",
    "│                            ↓                                    │\n",
    "│  DECODER (Unidirectional LSTM with Attention)                   │\n",
    "│  ┌─────────────────────────────────────────────────────────┐    │\n",
    "│  │  Input: <SOS> token, then previous predictions           │    │\n",
    "│  │  Attention: Bahdanau (additive) over encoder outputs    │    │\n",
    "│  │  LSTM: 2 layers × 512 hidden                            │    │\n",
    "│  │  Output: character probabilities for each step           │    │\n",
    "│  └─────────────────────────────────────────────────────────┘    │\n",
    "│                            ↓                                    │\n",
    "│  Output: \"გამარჯობა\" (corrected)                                │\n",
    "└─────────────────────────────────────────────────────────────────┘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade0f877",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Key Components Explained\n",
    "\n",
    "#### 1. Embedding Layer ([DL_HW_3 (1).ipynb](DL_HW_3%20(1).ipynb#L387-L389))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e2328",
   "metadata": {},
   "source": [
    "- Converts character indices to 256-dimensional dense vectors\n",
    "- `padding_idx=0` ensures `<PAD>` tokens don't affect gradients\n",
    "\n",
    "#### 2. Bidirectional LSTM Encoder ([DL_HW_3 (1).ipynb](DL_HW_3%20(1).ipynb#L390-L396))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d301589",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.lstm = nn.LSTM(\n",
    "    embedding_dim, hidden_dim, num_layers=2,\n",
    "    batch_first=True, dropout=dropout, bidirectional=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d95ad0",
   "metadata": {},
   "source": [
    "- **Bidirectional**: Reads the corrupted word both left→right AND right→left\n",
    "- This captures full context — knowing `რჯობა` comes after helps identify that `გაამ` should be `გამა`\n",
    "\n",
    "#### 3. Bahdanau Attention ([DL_HW_3 (1).ipynb](DL_HW_3%20(1).ipynb#L516-L548))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a488f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        # Computes attention weights over encoder outputs\n",
    "        energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], dim=2)))\n",
    "        attention_weights = torch.softmax(attention, dim=1)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b86cc5e",
   "metadata": {},
   "source": [
    "- **Why attention?** When generating the 3rd output character, the model can \"look at\" specific positions in the input\n",
    "- For spelling correction, attention learns to align corrupted↔correct characters\n",
    "\n",
    "#### 4. Decoder with Teacher Forcing ([DL_HW_3 (1).ipynb](DL_HW_3%20(1).ipynb#L630-L642))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525969b",
   "metadata": {},
   "source": [
    "one of the techniques we have covered pre-midterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9576d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# During training: feed ground truth tokens\n",
    "for t in range(tgt_len):\n",
    "    tgt_t = tgt[:, t].unsqueeze(1)  # Ground truth token\n",
    "    prediction, hidden, cell, _ = self.decoder(tgt_t, hidden, cell, encoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5870a",
   "metadata": {},
   "source": [
    "- **Teacher forcing**: During training, feed the correct previous character (not the model's prediction)\n",
    "- This speeds up training but creates exposure bias (addressed by inference logic)\n",
    "\n",
    "### How Character-Level RNNs Work Step-by-Step\n",
    "\n",
    "For input `\"გაა\"` → `\"გამ\"`:\n",
    "\n",
    "| Time | Input Token | LSTM State | Attention Focus | Output |\n",
    "|------|-------------|------------|-----------------|--------|\n",
    "| t=0  | `<SOS>` | Initial (from encoder) | Full input | `გ` |\n",
    "| t=1  | `გ` | Updated | Position 1-2 | `ა` |\n",
    "| t=2  | `ა` | Updated | Position 2-3 | `მ` |\n",
    "| t=3  | `მ` | Updated | - | `<EOS>` |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Data Quality and Thoughtful Dataset Construction\n",
    "\n",
    "### Data Source\n",
    "The data comes from data/wordsChunk_*.json files containing Georgian words.\n",
    "\n",
    "### Corruption Strategy — Simulating Real Typos\n",
    "\n",
    "The `corrupt_word` function ([DL_HW_3 (1).ipynb](DL_HW_3%20(1).ipynb#L214-L306)) implements **linguistically-informed data augmentation**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b148a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_type = random.choices(\n",
    "    ['substitute', 'delete', 'insert', 'transpose', 'repeat'],\n",
    "    weights=[0.35, 0.25, 0.20, 0.15, 0.05]\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9440ca6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Error Type | Weight | Real-World Basis |\n",
    "|------------|--------|------------------|\n",
    "| **Substitution** | 35% | Adjacent key typos (most common) |\n",
    "| **Deletion** | 25% | Skipping a letter while typing fast |\n",
    "| **Insertion** | 20% | Double-pressing or extra keystrokes |\n",
    "| **Transposition** | 15% | Swapping adjacent letters (\"teh\" → \"the\") |\n",
    "| **Repetition** | 5% | Accidental key hold (\"helllo\") |\n",
    "\n",
    "### Georgian Keyboard Layout Knowledge\n",
    "\n",
    "The code includes a **Georgian keyboard adjacency map** ([DL_HW_3 (1).ipynb](DL_HW_3%20(1).ipynb#L62-L98)):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e829094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEORGIAN_KEYBOARD = {\n",
    "   \"ა\": ['ქ','ს','ზ'],  # Keys adjacent to 'ა' on Georgian keyboard\n",
    "   'ბ': ['ვ','ნ','გ','ჰ'],\n",
    "   # ... etc\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f218bb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This ensures substitution errors are **realistic** — a typo for `ა` is likely `ქ` or `ს` (adjacent keys), not `ჰ` (opposite side of keyboard).\n",
    "\n",
    "### Why 100% Corruption Rate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b069ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRUPTION_RATE = 1.0  # 100% - ALL words get corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876a93c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The dataset uses 100% corruption because:\n",
    "1. The model needs to learn **correction**, not just copying\n",
    "2. Some \"clean\" examples (10% of failed corruptions) are kept for regularization\n",
    "3. treid other percentages but failed miserable\n",
    "---\n",
    "\n",
    "## 4. Full Lifecycle: Raw Data to Deployed Model\n",
    "\n",
    "### Phase 1: Data Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9c5261",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "Raw JSON words → Filter (len >= 2) → Corrupt 100% → (corrupted, correct) pairs\n",
    "     ↓\n",
    "├── wordsChunk_0.json\n",
    "├── wordsChunk_1.json        →    Training pairs: [(გაამრჯობა, გამარჯობა), ...]\n",
    "└── wordsChunk_2.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81053e6c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Phase 2: Vocabulary Building ([DL_HW_3 (1).ipynb](DL_HW_3%20(1).ipynb#L325-L377))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1643fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharVocab:\n",
    "    def __init__(self):\n",
    "        self.char2idx = {\n",
    "            '<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c0364a",
   "metadata": {},
   "source": [
    "- Special tokens: `<PAD>` (padding), `<SOS>` (start), `<EOS>` (end), `<UNK>` (unknown)\n",
    "- Build mapping from all characters in dataset → integer indices\n",
    "- Saved to char_vocab.json for inference\n",
    "\n",
    "### Phase 3: Training Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e201a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb75a6",
   "metadata": {},
   "source": [
    "**i want to thank two legends that were discussing memory usage of the words during the lecture for the fp16 idea**\n",
    "\n",
    "**Key training techniques:**\n",
    "\n",
    "1. **Mixed Precision (FP16)** — 2x speedup, lower memory:\n",
    "   ```python\n",
    "   with autocast():\n",
    "       output = model(src, tgt_input, ...)\n",
    "   scaler.scale(loss).backward()\n",
    "   ```\n",
    "\n",
    "2. **Label Smoothing** — Better generalization:\n",
    "   ```python\n",
    "   criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "   ```\n",
    "\n",
    "3. **Gradient Clipping** — Prevent exploding gradients:\n",
    "   ```python\n",
    "   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "   ```\n",
    "\n",
    "4. **Learning Rate Scheduling** — Reduce LR on plateau:\n",
    "   ```python\n",
    "   scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=2)\n",
    "   ```\n",
    "\n",
    "5. **Early Stopping** — Prevent overfitting:\n",
    "   ```python\n",
    "   if patience_counter >= max_patience:\n",
    "       break\n",
    "   ```\n",
    "\n",
    "### Phase 4: Model Saving\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5390fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'val_loss': best_val_loss,\n",
    "}, 'best_model1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e26a02",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Phase 5: Inference/Deployment (inferece.ipynb)\n",
    "\n",
    "The inference notebook shows the full deployment:\n",
    "\n",
    "1. **Load vocabulary** from char_vocab.json\n",
    "2. **Load model weights** from best_model1.pt\n",
    "3. **Greedy decoding** with safeguards:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a484a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_word(model, word, vocab, device='cuda', max_len=100):\n",
    "    # Encode input\n",
    "    src = torch.LongTensor([vocab.encode(word, add_sos=False, add_eos=True)])\n",
    "    \n",
    "    # Decode autoregressively\n",
    "    for step in range(max_len):\n",
    "        prediction, hidden, cell, _ = model.decoder(...)\n",
    "        next_token_id = prediction.argmax(dim=-1).item()\n",
    "        \n",
    "        # Stop conditions\n",
    "        if next_token_id == vocab.char2idx['<EOS>']: break\n",
    "        if len(decoded_tokens) > len(word) * 3: break  # Loop prevention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe7f04c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Aspect | Implementation | Rationale |\n",
    "|--------|----------------|-----------|\n",
    "| **Problem Framing** | Seq2Seq (corrupted → correct) | Variable-length I/O, alignment needed |\n",
    "| **Granularity** | Character-level | Small vocab (39), errors are character-level |\n",
    "| **Encoder** | 2-layer BiLSTM (256 hidden) | Captures full bidirectional context |\n",
    "| **Decoder** | 2-layer LSTM (512 hidden) | Larger capacity for generation |\n",
    "| **Attention** | Bahdanau (additive) | Soft alignment between I/O sequences |\n",
    "| **Data Augmentation** | Keyboard-aware corruption | Realistic typo simulation |\n",
    "| **Training** | FP16, label smoothing, early stopping | Efficient and generalizable |\n",
    "| **Inference** | Greedy decoding with safeguards | Prevents loops, handles edge cases |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
